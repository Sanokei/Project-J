{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI powered Highlight Clipper\n",
    "Use the whisper API to get transcipts and then use ChatGPT for sentiment analysis to get if its interesting or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "%pip install natsort\n",
    "%pip install yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "import pytube\n",
    "import yt_dlp\n",
    "\n",
    "import requests\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "\n",
    "import openai\n",
    "\n",
    "from natsort import natsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = config.OAI_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YouTube video URL\n",
    "vid_url = \"https://www.youtube.com/watch?v=oUrdoDs4q2Y\"\n",
    "\n",
    "# how long the testing clips should be\n",
    "testing_clips_time = 20\n",
    "\n",
    "# How high the sentiment has to be for it to be in the video\n",
    "sentiment_score = 0.6\n",
    "max_token_count = 3000\n",
    "custom_prompt = \"evaluate each sentence's entertainment value for a casual YouTube viewer\"\n",
    "\n",
    "# debug\n",
    "delete_directories = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the audio stream\n",
    "try:\n",
    "    video_info = yt_dlp.YoutubeDL({\"format\": \"bestaudio[ext!=webm]/best[ext!=webm]\"}).extract_info(vid_url, download=False)\n",
    "    video_stream = video_info['formats'][0]\n",
    "except:\n",
    "    video_info = yt_dlp.YoutubeDL({\"format\": \"bestvideo[ext!=webm]+bestaudio[ext!=webm]/best[ext!=webm]\"}).extract_info(vid_url, download=False)\n",
    "    video_stream = video_info['formats'][0]\n",
    "\n",
    "# Check if audio stream is not None before attempting to download it\n",
    "if video_stream is not None:\n",
    "    # Check if the file extension is a typical audio file format\n",
    "    file_ext = video_stream['ext']\n",
    "    if file_ext not in ['mp3', 'm4a', 'aac', 'wav', 'flac', 'ogg', 'opus']:\n",
    "        file_ext = 'mp3'\n",
    "    \n",
    "    # Download the audio stream\n",
    "    output_file_name = 'input.' + file_ext\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio[ext!=webm]/best[ext!=webm]',\n",
    "        'outtmpl': output_file_name,\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([vid_url])\n",
    "else:\n",
    "    print(\"No audio stream found.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break the video down into 25mb chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_video_by_seconds(input_file_path, chunk_duration):\n",
    "    \"\"\"\n",
    "    Splits a video file into smaller chunks using ffmpeg.\n",
    "    \"\"\"\n",
    "    # Get the duration of the input video\n",
    "    duration_command = [\"ffprobe\", \"-i\", input_file_path, \"-show_entries\", \"format=duration\", \"-v\", \"quiet\", \"-of\", \"csv=p=0\"]\n",
    "    duration_output = subprocess.check_output(duration_command, encoding=\"utf-8\")\n",
    "    input_duration = float(duration_output.strip())\n",
    "\n",
    "    # Calculate the number of chunks and their start and end times\n",
    "    chunk_start_times = range(0, int(input_duration), int(chunk_duration))\n",
    "    chunk_end_times = [min(start_time + chunk_duration, input_duration) for start_time in chunk_start_times]\n",
    "\n",
    "    # Split the video into chunks using ffmpeg\n",
    "    for i, (start_time, end_time) in enumerate(zip(chunk_start_times, chunk_end_times)):\n",
    "        output_file_path = f\"downloads/input_{i}.mp4\"\n",
    "        split_command = [\"ffmpeg\", \"-i\", input_file_path, \"-ss\", str(start_time), \"-to\", str(end_time), \"-c\", \"copy\", output_file_path]\n",
    "        subprocess.run(split_command, check=True)\n",
    "\n",
    "newpath = os.getcwd() + \"/downloads/\"\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "split_video_by_seconds(output_file_name, testing_clips_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create metadata for the files <br/>\n",
    "<sub>Start time and duration</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = os.getcwd() + \"/metadata/\"\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "start_time = 0.0\n",
    "\n",
    "for i, filename in enumerate(os.listdir('downloads')):\n",
    "    md_output_file_path = f\"metadata/input_{i}.txt\"\n",
    "    with open(md_output_file_path, 'w') as output_file:\n",
    "        clip = AudioFileClip('downloads/'+filename)\n",
    "        duration = clip.duration\n",
    "        clip.close()\n",
    "        output_file.write(f\"[{start_time},{duration}]\")\n",
    "        start_time += duration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input the audio into Whisper and get transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = os.getcwd() + \"/sentences/\"\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "url = 'https://api.openai.com/v1/audio/transcriptions'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {config.OAI_API_TOKEN}'\n",
    "}\n",
    "data = {\n",
    "    'model':'whisper-1',\n",
    "    'prompt':'Woah! I for a fact don\\'t believe that... Okay, I know. I love those~ [silence]. [music]. [ingame_sound]'\n",
    "}\n",
    "for chunk_num, filename in enumerate(os.listdir('downloads')):\n",
    "    with open(\"downloads/\" + filename, \"rb\") as input_file:\n",
    "        files = {\n",
    "            'file':(filename,input_file)\n",
    "        }\n",
    "        transcription = requests.post(url=url, headers=headers, files=files, data=data)\n",
    "        output_file_path = f\"sentences/input_{chunk_num}.txt\"\n",
    "        with open(output_file_path, 'w', encoding=\"utf-8\") as output_file:\n",
    "            output_file.write(transcription.json().get('text'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.openai.com/v1/chat/completions'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {config.OAI_API_TOKEN}'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakdown the text to the max_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = os.getcwd() + \"/temporary/\"\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "# Initialize chunk_num variable\n",
    "chunk_num = 0\n",
    "\n",
    "# Loop through os.listdir('sentences')\n",
    "for filename in natsorted(os.listdir(\"sentences\")):\n",
    "    with open(os.path.join(\"sentences\", filename), \"r\", encoding=\"utf-8\") as chunk_file, \\\n",
    "         open(os.path.join(\"metadata\", filename), \"r\", encoding=\"utf-8\") as timestamp_file:\n",
    "\n",
    "        # Create a new file in the newly created temporary folder\n",
    "        temporary_file_path = os.path.join(os.getcwd() + \"/temporary/\", f\"input_{chunk_num}.txt\")\n",
    "\n",
    "        # Initialize word count\n",
    "        word_count = 0\n",
    "        with open(temporary_file_path, \"a\", encoding=\"utf-8\") as temp_file:\n",
    "                temp_file.write(\"[\")\n",
    "\n",
    "        # Get metadata and sentence\n",
    "        zipped_content = list(zip(timestamp_file, chunk_file))\n",
    "        for i, (metadata, sentence) in enumerate(zipped_content):\n",
    "            # Split sentence into words\n",
    "            words = sentence.split()\n",
    "\n",
    "            # Check if adding the current sentence would exceed the 3000 words limit\n",
    "            if word_count + len(words) > max_token_count:\n",
    "                with open(temporary_file_path, 'rb+') as filehandle:\n",
    "                    filehandle.seek(-1, os.SEEK_END)\n",
    "                    filehandle.truncate()\n",
    "                with open(temporary_file_path, \"a\") as temp_file:\n",
    "                    temp_file.write(\"]\")\n",
    "                # Increment chunk_num and create a new file\n",
    "                chunk_num += 1\n",
    "                temporary_file_path = os.path.join(os.getcwd() + \"/temporary/\", f\"input_{chunk_num}.txt\")\n",
    "                with open(temporary_file_path, \"a\", encoding=\"utf-8\") as temp_file:\n",
    "                    temp_file.write(\"[\")\n",
    "                word_count = 0\n",
    "\n",
    "            # Add metadata and sentence to the file\n",
    "            with open(temporary_file_path, \"a\", encoding=\"utf-8\") as temp_file:\n",
    "                # Add a comma if not the first element in the file (to avoid extra comma at the end)\n",
    "                temp_file.write(f\"[{metadata.strip()},\\\"{sentence.strip()}\\\"],\")\n",
    "\n",
    "            # Update word_count\n",
    "            word_count += len(words)\n",
    "            \n",
    "with open(temporary_file_path, 'rb+') as filehandle:\n",
    "    filehandle.seek(-1, os.SEEK_END)\n",
    "    filehandle.truncate()\n",
    "with open(temporary_file_path, \"a\") as temp_file:\n",
    "    temp_file.write(\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ChatGPT to break down the text and meta data into workable chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = os.getcwd() + \"/video_transcription/\"\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "for chunk_num, filename in enumerate(os.listdir('temporary')):\n",
    "    with open(f\"temporary/input_{chunk_num}.txt\",\"r\", encoding=\"utf-8\") as input_file:\n",
    "        data = {\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": f\"Using this list of [start_time, duration, video_transcription]. Respond with only a new list of chunks of the text you as an AI model find appropriately fits in speaking english by combinding list elements and then getting the new start_time by getting the first start_time of that chunk, then get the duration by adding the duration times. Get the new start and duration together to create the new list of [start_time, duration, chunk]. DO NOT give any answer outisde of the format. \\n\\\"{input_file.read()}\\\"\"}] \n",
    "        }\n",
    "        response = requests.post(url=url, headers=headers, json=data)\n",
    "        created_chunks = response.json().get('choices')[0].get('message').get('content')\n",
    "        with open(f\"video_transcription/input_{chunk_num}.txt\", \"w\", encoding=\"utf-8\") as vidtrans:\n",
    "            vidtrans.write(created_chunks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get sentiment from ChatGPT and create sentiment analysis file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = os.getcwd() + \"/sentiment_analysis/\"\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "for chunk_num, filename in enumerate(os.listdir('video_transcription')):\n",
    "    with open(f\"video_transcription/input_{chunk_num}.txt\", \"r\", encoding=\"utf-8\") as input_file:\n",
    "        data = {\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": f\"Given a list of video segments in the format [start_time, duration, sentence]. {custom_prompt}. Assign a float score representing the entertainment value, with higher values indicating more entertaining content. Provide the results as a list in the format [start_time, duration, sentence, float_score]. DO NOT give any answer outisde of the format. \\n\\\"{input_file.read()}\\\"\"}] \n",
    "        }\n",
    "        response = requests.post(url=url, headers=headers, json=data)\n",
    "        sentiment = response.json().get('choices')[0].get('message').get('content')\n",
    "        with open(f\"sentiment_analysis/input_{chunk_num}.txt\",\"w\", encoding=\"utf-8\") as sentanal: # hehe\n",
    "            sentanal.write(sentiment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Video from choosen segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydl_opts = {\n",
    "    'format': 'bestvideo[ext=avi]+bestaudio[ext=m4a]/best[ext=avi]/best',\n",
    "    'outtmpl': 'input_video.avi'\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([vid_url])\n",
    "\n",
    "# Run ffprobe command to get video metadata\n",
    "ffprobe_command = [\n",
    "    'ffprobe',\n",
    "    '-v', 'quiet',\n",
    "    '-print_format', 'json',\n",
    "    '-show_streams',\n",
    "    'input_video.avi'\n",
    "]\n",
    "\n",
    "# Capture the output of the ffprobe command\n",
    "result = subprocess.run(ffprobe_command, capture_output=True, text=True)\n",
    "\n",
    "# Parse the JSON output\n",
    "metadata = json.loads(result.stdout)\n",
    "\n",
    "# Extract the fps from the metadata\n",
    "fps = None\n",
    "for stream in metadata['streams']:\n",
    "    if stream['codec_type'] == 'video':\n",
    "        # Calculate fps from r_frame_rate (numerator/denominator)\n",
    "        numerator, denominator = map(int, stream['r_frame_rate'].split('/'))\n",
    "        fps = numerator / denominator\n",
    "        break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = os.getcwd() + \"/videos/\"\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "for chunk_num, filename in enumerate(os.listdir('sentiment_analysis')):\n",
    "    with open(f\"sentiment_analysis/input_{chunk_num}.txt\",\"r\", encoding=\"utf-8\") as sentanal:\n",
    "        for i, sentlist in enumerate(ast.literal_eval(sentanal.read())):\n",
    "            if(float(sentlist[3]) >= sentiment_score):\n",
    "                output_file_path = f\"videos/output_{i}.avi\"\n",
    "                split_command = [\"ffmpeg\", \"-i\", \"input_video.avi\", \"-framerate\", str(fps), \"-ss\", str(sentlist[0]), \"-t\", str(sentlist[1]), \"-c:v\", \"libx264\", \"-crf\", \"23\", \"-c:a\", \"copy\", output_file_path]\n",
    "                try:\n",
    "                    subprocess.run(split_command, check=True, capture_output=True, text=True)\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    print(f\"Error occurred while running command: {e}\")\n",
    "                    print(f\"Output: {e.output}\")\n",
    "                    print(f\"Error output: {e.stderr}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of commands to concatenate the videos\n",
    "commands = ['ffmpeg', '-i', f'concat:{\"|\".join([\"videos/\" + x for x in natsorted(os.listdir(\"videos\"))])}', '-c', 'copy', 'output.avi']\n",
    "\n",
    "# Run the ffmpeg command and capture the output and error messages\n",
    "subprocess.run(commands, capture_output=True, text=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(delete_directories):\n",
    "    newpath = [\"/metadata/\",\"/downloads/\",\"/sentences/\",\"/videos/\",\"/video_transcription\",\"/sentiment_analysis\",\"/temporary/\"]\n",
    "    for i in newpath:\n",
    "        if os.path.exists(os.getcwd() + i):\n",
    "            shutil.rmtree(os.getcwd() + i)\n",
    "\n",
    "    newfilepath = [output_file_name,\"/input_video.avi\"]\n",
    "    for i in newfilepath:\n",
    "        if os.path.exists(os.getcwd() + i):\n",
    "            os.remove(os.getcwd() + i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
