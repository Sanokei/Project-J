{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dataset\n",
    "Use the whisper API to get transcipts and then use ChatGPT to get if its interesting or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pytube\n",
    "%pip install youtube-dl\n",
    "%pip install openai\n",
    "%pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "import pytube\n",
    "\n",
    "import requests\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "\n",
    "import openai\n",
    "\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "from moviepy.audio.io.AudioFileClip import AudioFileClip\n",
    "\n",
    "import youtube_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = config.OAI_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YouTube video URL\n",
    "vid_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n",
    "fps = 30\n",
    "\n",
    "# how long the testing clips should be\n",
    "testing_clips_time = 20\n",
    "\n",
    "# How high the sentiment has to be for it to be in the video\n",
    "sentiment_score = 0.6\n",
    "\n",
    "# debug\n",
    "delete_directories = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the audio stream\n",
    "video_stream = pytube.YouTube(vid_url).streams.filter(only_audio=True).first()\n",
    "# Check if audio stream is not None before attempting to download it\n",
    "if video_stream is not None:\n",
    "    # Download the audio stream\n",
    "    audio_file = video_stream.download(filename=\"input.mp4\").replace(\"\\\\\",\"/\")\n",
    "else:\n",
    "    print(\"No audio stream found.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break the video down into 25mb chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_video_by_seconds(input_file_path, chunk_duration):\n",
    "    \"\"\"\n",
    "    Splits a video file into smaller chunks using ffmpeg.\n",
    "    \"\"\"\n",
    "    # Get the duration of the input video\n",
    "    duration_command = [\"ffprobe\", \"-i\", input_file_path, \"-show_entries\", \"format=duration\", \"-v\", \"quiet\", \"-of\", \"csv=p=0\"]\n",
    "    duration_output = subprocess.check_output(duration_command, encoding=\"utf-8\")\n",
    "    input_duration = float(duration_output.strip())\n",
    "\n",
    "    # Calculate the number of chunks and their start and end times\n",
    "    chunk_start_times = range(0, int(input_duration), int(chunk_duration))\n",
    "    chunk_end_times = [min(start_time + chunk_duration, input_duration) for start_time in chunk_start_times]\n",
    "\n",
    "    # Split the video into chunks using ffmpeg\n",
    "    for i, (start_time, end_time) in enumerate(zip(chunk_start_times, chunk_end_times)):\n",
    "        output_file_path = f\"downloads/input_{i}.mp4\"\n",
    "        split_command = [\"ffmpeg\", \"-i\", input_file_path, \"-ss\", str(start_time), \"-to\", str(end_time), \"-c\", \"copy\", output_file_path]\n",
    "        subprocess.run(split_command, check=True)\n",
    "        \n",
    "def split_file(input_file_path, chunk_size):\n",
    "    \"\"\"\n",
    "    Splits a file into smaller chunks and saves them to disk.\n",
    "    \"\"\"\n",
    "    with open(input_file_path, 'rb') as input_file:\n",
    "        for chunk_num, chunk in enumerate(iter(lambda: input_file.read(chunk_size), b'')):\n",
    "            # Save the chunk to disk\n",
    "            output_file_path = f\"downloads/input_{chunk_num}.mp4\"\n",
    "            with open(output_file_path, 'wb') as output_file:\n",
    "                output_file.write(chunk)\n",
    "\n",
    "newpath = os.getcwd() + \"/downloads/\"\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "    \n",
    "input_file_path = 'input.mp4'\n",
    "split_video_by_seconds(input_file_path, testing_clips_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create metadata for the files <br/>\n",
    "<sub>Start time and duration</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = os.getcwd() + \"/metadata/\"\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "start_time = 0.0\n",
    "\n",
    "for i, filename in enumerate(os.listdir('downloads')):\n",
    "    md_output_file_path = f\"metadata/input_{i}.txt\"\n",
    "    with open(md_output_file_path, 'w') as output_file:\n",
    "        clip = AudioFileClip('downloads/'+filename)\n",
    "        duration = clip.duration\n",
    "        clip.close()\n",
    "        output_file.write(f\"[{start_time},{duration}]\")\n",
    "        start_time += duration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input the audio into Whisper and get transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = os.getcwd() + \"/chunks/\"\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "url = 'https://api.openai.com/v1/audio/transcriptions'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {config.OAI_API_TOKEN}'\n",
    "}\n",
    "data = {\n",
    "    'model':'whisper-1',\n",
    "    'prompt':'Woah! I for a fact don\\'t believe that... Okay, I know. I love those~'\n",
    "}\n",
    "for chunk_num, filename in enumerate(os.listdir('downloads')):\n",
    "    with open(\"downloads/\" + filename, \"rb\") as input_file:\n",
    "        files = {\n",
    "            'file':(filename,input_file)\n",
    "        }\n",
    "        transcription = requests.post(url=url, headers=headers, files=files, data=data)\n",
    "        output_file_path = f\"chunks/input_{chunk_num}.txt\"\n",
    "        with open(output_file_path, 'w', encoding=\"utf-8\") as output_file:\n",
    "            output_file.write(transcription.json().get('text'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.openai.com/v1/chat/completions'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {config.OAI_API_TOKEN}'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ChatGPT to break down the text and meta data into workable chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('chunks'):\n",
    "    with open(\"temporary.txt\", \"a\", encoding=\"utf-8\") as temporary:\n",
    "        with open(\"chunks/\" + filename, \"r\", encoding=\"utf-8\") as chunk:\n",
    "            with open(\"metadata/\" + filename, \"r\", encoding=\"utf-8\") as timestamp:\n",
    "                temporary.write(f\"[{timestamp.read()},\\\"{chunk.read()}\\\"],\")\n",
    "\n",
    "with open(\"temporary.txt\",\"r\", encoding=\"utf-8\") as input_file:\n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": f\"Using this list of [start_time, duration, video_transcription]. Respond with only a new list of chunks of the text you as an AI model find appropriately fits in speaking english by combinding list elements and then adding the start time and duration together to create the new list of [start_time, duration, chunk]. DO NOT give any answer outisde of the format. \\n\\\"{input_file.read()}\\\"\"}] \n",
    "    }\n",
    "    response = requests.post(url=url, headers=headers, json=data)\n",
    "    created_chunks = response.json().get('choices')[0].get('message').get('content')\n",
    "    with open(\"video_transcription.txt\", \"w\", encoding=\"utf-8\") as vidtrans:\n",
    "        vidtrans.write(created_chunks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get sentiment from ChatGPT and create sentiment analysis file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"video_transcription.txt\", \"r\", encoding=\"utf-8\") as input_file:\n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": f\"Using the list [start_time, duration, sentence]. For how 'entertaining a casual viewer of Youtube would find the sentence as part of a video' in any definition to the best of your ability as an AI model the sentence is create a score as a float value to the best of your ability as an AI model. Only respond with a new list of [start_time, duration, sentence, float_score]. DO NOT give any answer outisde of the format. \\n\\\"{input_file.read()}\\\"\"}] \n",
    "    }\n",
    "    response = requests.post(url=url, headers=headers, json=data)\n",
    "    sentiment = response.json().get('choices')[0].get('message').get('content')\n",
    "    with open(\"sentiment_analysis.txt\",\"w\", encoding=\"utf-8\") as sentanal: # hehe\n",
    "        sentanal.write(sentiment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Video from choosen segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the audio stream\n",
    "video_stream = pytube.YouTube(vid_url).streams.filter().first()\n",
    "# Check if audio stream is not None before attempting to download it\n",
    "if video_stream is not None:\n",
    "    # Download the audio stream\n",
    "    video_file = video_stream.download(filename=\"input_video.mp4\").replace(\"\\\\\",\"/\")\n",
    "else:\n",
    "    print(\"No audio stream found.\")\n",
    "\n",
    "newpath = os.getcwd() + \"/videos/\"\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "with open(\"sentiment_analysis.txt\",\"r\", encoding=\"utf-8\") as sentanal: # again hehe\n",
    "    for i, sentlist in enumerate(ast.literal_eval(sentanal.read())):\n",
    "        if(float(sentlist[3]) >= sentiment_score):\n",
    "            output_file_path = f\"videos/output_{i}.mp4\"\n",
    "            split_command = [\"ffmpeg\", \"-i\", \"input_video.mp4\", \"-ss\", str(sentlist[0]), \"-t\", str(sentlist[1]), \"-c\", \"copy\", output_file_path]\n",
    "            subprocess.run(split_command, check=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of video files to combine\n",
    "video_files = os.listdir('videos')\n",
    "# Create a list of commands to concatenate the videos\n",
    "commands = ['ffmpeg', '-i', f'concat:{\"|\".join(\"videos/\" + video_files_with_path)}', '-c', 'copy', 'output.mp4']\n",
    "\n",
    "# Run the ffmpeg command\n",
    "subprocess.call(commands)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(delete_directories):\n",
    "    newpath = [\"/metadata/\",\"/downloads/\",\"/chunks/\",\"/videos/\"]\n",
    "    for i in newpath:\n",
    "        if os.path.exists(os.getcwd() + i):\n",
    "            shutil.rmtree(os.getcwd() + i)\n",
    "\n",
    "    newfilepath = [\"/temporary.txt\", \"/video_transcription.txt\",\"/sentiment_analysis.txt\",\"/input.mp4\",\"/input_video.mp4\"]\n",
    "    for i in newfilepath:\n",
    "        if os.path.exists(os.getcwd() + i):\n",
    "            os.remove(os.getcwd() + i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
